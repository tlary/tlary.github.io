<!DOCTYPE html>
<html lang="en" data-theme=""><head>
    <title> Tobias Larysch | Kaggle Challenge: House Prices </title>

    
    <meta charset="utf-8"><meta name="generator" content="Hugo 0.74.3" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
    <meta name="description" content="Economist and Data Enthusiast">
    
    <link rel="stylesheet"
          href="../../css/style.min.9a6700e4461b50dccdddfc4f81dc65d77e7fca22c35665e398a0c36568db59c7.css"
          integrity="sha256-mmcA5EYbUNzN3fxPgdxl135/yiLDVmXjmKDDZWjbWcc="
          crossorigin="anonymous"
          type="text/css">
    
    <link rel="stylesheet"
        href="../../css/markupHighlight.min.9755453ffb7bc4cd220f86ebb5922107b49f193cc62fc17e9785d27b33a8bf5b.css"
        integrity="sha256-l1VFP/t7xM0iD4brtZIhB7SfGTzGL8F&#43;l4XSezOov1s="
        crossorigin="anonymous"
        type="text/css">
    
        
        
        <link rel="stylesheet"
        href="../../css/custom4.min.0b1de9c512ef08638b8b56b83c45de26dbc6bc978b6d9cdfd89fbf3903b8bcbb.css"
        integrity="sha256-Cx3pxRLvCGOLi1a4PEXeJtvGvJeLbZzf2J&#43;/OQO4vLs="
        crossorigin="anonymous"
        media="screen" />
    
        
        
        <link rel="stylesheet"
        href="../../css/friend.min.d0809843e4028aaa20decda8dda26d1a3bae5e47e87311da5a10b607f015390a.css"
        integrity="sha256-0ICYQ&#43;QCiqog3s2o3aJtGjuuXkfocxHaWhC2B/AVOQo="
        crossorigin="anonymous"
        media="screen" />
    
    <link rel="stylesheet" 
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" 
    integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" 
    crossorigin="anonymous" />

    
    <link rel="shortcut icon" href="../../favicons/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" sizes="180x180" href="../../favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicons/favicon-16x16.png">

    <link rel="canonical" href="../../post/house_prices/">

    
    
    
    
    <script type="text/javascript"
            src="../../js/anatole-header.min.d8599ee07b7d3f11bafbac30657ccc591e8d7fd36a9f580cd4c09e24e0e4a971.js"
            integrity="sha256-2Fme4Ht9PxG6&#43;6wwZXzMWR6Nf9Nqn1gM1MCeJODkqXE="
            crossorigin="anonymous"></script>


    <script type="text/javascript"
            src="../../js/custom.min.adead2e63eefe548b5ce0aa68303df62a2e2e975242f58d58c080fb3a61e11d7.js"
            integrity="sha256-rerS5j7v5Ui1zgqmgwPfYqLi6XUkL1jVjAgPs6YeEdc="
            crossorigin="anonymous"></script>
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kaggle Challenge: House Prices"/>
<meta name="twitter:description" content="This blogpost contains code to the Kaggle House Price Challenge. The aim of this blogpost is to get familiar with the most commonly used Python data science stack - data preprocessing and cleaning with pandas and numpy, model building and evaluation using scikit-learn and different algorithms."/>


    
	<script async src="https://cdn.panelbear.com/analytics.js?site=cIJFGgu2o5"></script>
	<script>
    	window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    	panelbear('config', { site: 'cIJFGgu2o5'});
	</script>


    

    
    <script>
    
    window.onload = function(){
    document
        .getElementById("Linkedin")
        .addEventListener("click", function (e) {
        e.preventDefault();

        
        panelbear("track", "LinkedIn");
        
        
        window.open(this.href, "_tab");
        
    });
    
    
    document
        .getElementById("GitHub")
        .addEventListener("click", function (e) {
        e.preventDefault();

        
        panelbear("track", "GitHub");
        
        
        window.open(this.href, "_blank");
        
    });
    


    

    
    document
        .getElementById("e-mail")
        .addEventListener("click", function (e) {
        e.preventDefault();

        
        panelbear("track", "Mail");
        
        
        window.open(this.href, "_blank");
        
    });
    
    
    document
        .getElementById("Xing")
        .addEventListener("click", function (e) {
        e.preventDefault();

        
        panelbear("track", "Xing");
        
        
        window.open(this.href, "_blank");
        
    });
    }
    </script>






</head>
<body><div class="sidebar . ">
    <div class="logo-title">
        <div class="title">
            <img src="../../images/foto_cropped_small.jpg" alt="profile picture">
            <h3 title=""><a href="../../">Tobias Larysch - Portfolio</a></h3>
            <div class="description">
                <p>Economist and Data Enthusiast</p>
            </div>
        </div>
    </div>
    <ul class="social-links">
        
            <li>
                <a href="https://www.linkedin.com/in/tobias-larysch-97981519b/" rel="me" aria-label="Linkedin" id="Linkedin">
                    <i class="fab fa-linkedin fa-2x" aria-hidden="true"></i>
                </a>
            </li>
        
            <li>
                <a href="https://github.com/tlary" rel="me" aria-label="GitHub" id="GitHub">
                    <i class="fab fa-github fa-2x" aria-hidden="true"></i>
                </a>
            </li>
        
            <li>
                <a href="mailto:tobias-larysch@gmx.net" rel="me" aria-label="e-mail" id="e-mail">
                    <i class="fas fa-envelope fa-2x" aria-hidden="true"></i>
                </a>
            </li>
        
            <li>
                <a href="https://www.xing.com/profile/Tobias_Larysch/cv" rel="me" aria-label="Xing" id="Xing">
                    <i class="fab fa-xing fa-2x" aria-hidden="true"></i>
                </a>
            </li>
        
    </ul>
    <div class="footer">
        <div class="by_farbox">&copy; Tobias Larysch  2021 </div>
    </div>
</div>
<div class="main">
    <div class="page-top  . ">
    <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
    </a>
    <ul class="nav" id="navMenu">
        
        
            
            <li><a 
                   href="../../"
                        
                   title="">Projects </a></li>
        
            
            <li><a 
                   href="../../publications/"
                        
                   title="">Publications</a></li>
        
        
        
    </ul>
</div>

    <div class="autopagerize_page_element">
        <div class="content">

    <div class="post  . ">
    	
    	
    		<nav id="TableOfContents">
  <ul>
    <li><a href="#1-data-preprocessing-and-cleaning">1. Data preprocessing and cleaning</a></li>
    <li><a href="#2-model-selection">2. Model selection</a>
      <ul>
        <li><a href="#21-k-nearest-neighbors-knn">2.1 k-nearest Neighbors (KNN)</a></li>
        <li><a href="#22-random-forest">2.2 Random forest</a></li>
      </ul>
    </li>
    <li><a href="#3-making-predictions-and-creating-the-submission-file">3. Making predictions and creating the submission file</a></li>
  </ul>
</nav>
        
    	
        <div class="post-content">
            
            <div class="post-title">
                <h2>Kaggle Challenge: House Prices</h3>
                
                    <div class="info">
                        <em class="fas fa-calendar-day"></em>
                        <span class="date"> 
                                                Thu, Jun 13, 2019
                                           </span>
                        <em class="fas fa-stopwatch"></em>
                        <span class="reading-time">11-minute read</span>
                        
                        
                            <em class="fab fa-github"></em>
                            <a href= https://github.com/tlary/Kaggle_house_prices>
                            <span>View on GitHub</span>
                            </a>
                        
                    </div>
                    <div>
                    	<hr style="height:1px;border-width:0;background-color:rgba(0,0,0,0.15)">
                    </div>
                
            </div>
            
            
	    <div class="post-content-text">
    	        <p>This blogpost contains code to the <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview">Kaggle House Price Challenge</a>. The aim of this blogpost is to get familiar with the most commonly used Python data science stack - data preprocessing and cleaning with pandas, model building and evaluation using scikit-learn and different algorithms.</p>
<p>First, we start by importing the libraries needed for this project.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</code></pre></div><p>Loading the data into dataframes.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;test.csv&#34;</span><span class="p">)</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;train.csv&#34;</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">train</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div><p>We can see that the test dataset contains one additional variable compared to the train dataset, which is the &ldquo;SalePrice&rdquo; variable. In our analysis / prediction this serves as the dependent variable we want to predict given the houses&rsquo; characteristics.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train</span><span class="o">.</span><span class="n">SalePrice</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div><h2 id="1-data-preprocessing-and-cleaning">1. Data preprocessing and cleaning</h2>
<p>We&rsquo;ll start the Data Cleaning by checking if the dependent variable in the test dataset contains any missing values.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train</span><span class="o">.</span><span class="n">SalePrice</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div><p>All observations contain data for the target variable, therefore we can continue by taking a look at all the other variables contained in the train and test dataset.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">miss_count_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">perc_miss_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">train</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">())</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">missings_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">miss_count_train</span><span class="p">,</span> <span class="n">perc_miss_train</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;Total&#34;</span><span class="p">,</span> <span class="s2">&#34;Percent&#34;</span><span class="p">])</span>

<span class="n">miss_count_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">perc_miss_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">test</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">())</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">missings_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">miss_count_test</span><span class="p">,</span> <span class="n">perc_miss_test</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;Total&#34;</span><span class="p">,</span> <span class="s2">&#34;Percent&#34;</span><span class="p">])</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">missings_train</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">missings_test</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>As a rule of thumb we completely ignore columns that contain at least 15% missing values and will not try to impute the missing values with any kind of computation, e.g. using means. Therefore, we will delete the variables &ldquo;PoolQC&rdquo;, &ldquo;MiscFeature&rdquo;, &ldquo;Alley&rdquo;, &ldquo;Fence&rdquo;, &ldquo;FireplaceQu&rdquo; and &ldquo;LotFrontage&rdquo;.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;PoolQC&#34;</span><span class="p">,</span> <span class="s2">&#34;MiscFeature&#34;</span><span class="p">,</span> <span class="s2">&#34;Alley&#34;</span><span class="p">,</span> <span class="s2">&#34;Fence&#34;</span><span class="p">,</span> <span class="s2">&#34;FireplaceQu&#34;</span><span class="p">,</span> <span class="s2">&#34;LotFrontage&#34;</span><span class="p">])</span>
</code></pre></div><p>The variables &ldquo;GarageCond&rdquo;, &ldquo;GarageType&rdquo;, &ldquo;GarageQual&rdquo;, &ldquo;GarageYrBlt&rdquo; and &ldquo;GarageFinish&rdquo; contain exactly the same number of missing values, which seems kind of odd. Therefore, we&rsquo;ll take a closer look at these variables.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&#34;GarageCond&#34;</span><span class="p">,</span> <span class="s2">&#34;GarageType&#34;</span><span class="p">,</span> <span class="s2">&#34;GarageQual&#34;</span><span class="p">,</span> <span class="s2">&#34;GarageYrBlt&#34;</span><span class="p">,</span> <span class="s2">&#34;GarageFinish&#34;</span><span class="p">]:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">train</span><span class="p">[</span><span class="n">var</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="s2">&#34;count&#34;</span><span class="p">))</span>
</code></pre></div><p>We can see that for &ldquo;GarageCond&rdquo; and &ldquo;GarageQual&rdquo; the most frequently occurring value is &ldquo;TA&rdquo;, which means that the condition and quality of the garages are average/typical. We will replace the missing values of these two variables therefore with &ldquo;TA&rdquo; as well. The variable &ldquo;GarageYrBlt&rdquo; refers to the year in which the garage was built. Since we also have the year in which the houses themselves are built, we can drop this variable without losing much explanatory information. In addition to that we also drop the &ldquo;GarageFinish&rdquo; and &ldquo;GarageType&rdquo; variable.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;GarageYrBlt&#34;</span><span class="p">,</span> <span class="s2">&#34;GarageFinish&#34;</span><span class="p">,</span> <span class="s2">&#34;GarageType&#34;</span><span class="p">])</span>
<span class="n">train</span><span class="p">[</span><span class="s2">&#34;GarageCond&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">GarageCond</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">&#34;TA&#34;</span><span class="p">)</span>
<span class="n">train</span><span class="p">[</span><span class="s2">&#34;GarageQual&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">GarageQual</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">&#34;TA&#34;</span><span class="p">)</span>
</code></pre></div><p>In the same way as above, we take a closer look at the &ldquo;Bsmt*&rdquo; variables.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&#34;BsmtFinType2&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtExposure&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtCond&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtFinType1&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtQual&#34;</span><span class="p">]:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">train</span><span class="p">[</span><span class="n">var</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="s2">&#34;count&#34;</span><span class="p">))</span>
</code></pre></div><p>We delete the &ldquo;BsmtFinType*&rdquo; variables since these are highly subjective and do not add much information to our model. The missing values of &ldquo;BsmtCond&rdquo; will be imputed with the most common value &ldquo;TA&rdquo;. The rows containing missing values for &ldquo;BsmtQual&rdquo; and &ldquo;BsmtExposure&rdquo; will be deleted from the dataset.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;BsmtFinType1&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtFinType2&#34;</span><span class="p">])</span>
<span class="n">train</span><span class="o">.</span><span class="n">BsmtCond</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s2">&#34;BsmtCond&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">&#34;TA&#34;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&#34;BsmtQual&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtExposure&#34;</span><span class="p">]:</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()]</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</code></pre></div><p>The variable &ldquo;Electrical&rdquo; contains only one missing value, therefore we only delete this specific row of data. We proceed in the same way with &ldquo;MasVnrType&rdquo; and &ldquo;MasVnrArea&rdquo;.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&#34;Electrical&#34;</span><span class="p">,</span> <span class="s2">&#34;MasVnrType&#34;</span><span class="p">,</span> <span class="s2">&#34;MasVnrArea&#34;</span><span class="p">]:</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()]</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</code></pre></div><p>Running the above code again to check if all missing values are deleted.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">miss_count_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">perc_miss_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">train</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">())</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">missings_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">miss_count_train</span><span class="p">,</span> <span class="n">perc_miss_train</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;Total&#34;</span><span class="p">,</span> <span class="s2">&#34;Percent&#34;</span><span class="p">])</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">missings_train</span><span class="o">.</span><span class="n">Percent</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div><pre><code>0
</code></pre>
<p>This looks good.</p>
<p>We now handled all the missing data on the training set. As the next step we will clean the test data. Since we want to evaluate our model on Kaggle after finishing the modeling, we cannot drop any observations because we need predicted house prices for each row of the test data. Therefore, we will impute the missing data in the test dataset with the most frequent category for categorical features and the mean for numerical features.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;PoolQC&#34;</span><span class="p">,</span> <span class="s2">&#34;MiscFeature&#34;</span><span class="p">,</span> <span class="s2">&#34;Alley&#34;</span><span class="p">,</span> <span class="s2">&#34;Fence&#34;</span><span class="p">,</span> <span class="s2">&#34;FireplaceQu&#34;</span><span class="p">,</span> <span class="s2">&#34;LotFrontage&#34;</span><span class="p">])</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;GarageYrBlt&#34;</span><span class="p">,</span> <span class="s2">&#34;GarageFinish&#34;</span><span class="p">,</span> <span class="s2">&#34;GarageType&#34;</span><span class="p">])</span>
<span class="n">test</span><span class="p">[</span><span class="s2">&#34;GarageCond&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">GarageCond</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">&#34;TA&#34;</span><span class="p">)</span>
<span class="n">test</span><span class="p">[</span><span class="s2">&#34;GarageQual&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">GarageQual</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">&#34;TA&#34;</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;BsmtFinType1&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtFinType2&#34;</span><span class="p">])</span>
<span class="n">test</span><span class="o">.</span><span class="n">BsmtCond</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s2">&#34;BsmtCond&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">&#34;TA&#34;</span><span class="p">)</span>

</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># categorical:</span>
<span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&#34;BsmtExposure&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtQual&#34;</span><span class="p">,</span> <span class="s2">&#34;MasVnrType&#34;</span><span class="p">,</span> <span class="s2">&#34;MSZoning&#34;</span><span class="p">,</span> <span class="s2">&#34;Utilities&#34;</span><span class="p">,</span> <span class="s2">&#34;Functional&#34;</span><span class="p">,</span> <span class="s2">&#34;SaleType&#34;</span><span class="p">,</span> <span class="s2">&#34;Exterior2nd&#34;</span><span class="p">,</span> <span class="s2">&#34;Exterior1st&#34;</span><span class="p">,</span> <span class="s2">&#34;KitchenQual&#34;</span><span class="p">]:</span>
    <span class="n">test</span><span class="p">[</span><span class="n">var</span><span class="p">]</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">test</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># numerical</span>
<span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&#34;MasVnrArea&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtHalfBath&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtFullBath&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtUnfSF&#34;</span><span class="p">,</span> <span class="s2">&#34;GarageArea&#34;</span><span class="p">,</span> <span class="s2">&#34;GarageCars&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtFinSF1&#34;</span><span class="p">,</span> <span class="s2">&#34;BsmtFinSF2&#34;</span><span class="p">,</span> <span class="s2">&#34;TotalBsmtSF&#34;</span><span class="p">]:</span>
    <span class="n">test</span><span class="p">[</span><span class="n">var</span><span class="p">]</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">miss_count_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">perc_miss_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">test</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">())</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">missings_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">miss_count_test</span><span class="p">,</span> <span class="n">perc_miss_test</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;Total&#34;</span><span class="p">,</span> <span class="s2">&#34;Percent&#34;</span><span class="p">])</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">missings_test</span><span class="o">.</span><span class="n">Percent</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div><pre><code>0
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Perfect. We do not have any missing values in our test set anymore and have kept all observations, for which we are going to predict the House Sale Price.</p>
<p>For now, we will not go deeply into feature engineering. The only additional variable besides the ones already in the dataset we are going to create is a variable containing the total area of the living space.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train</span><span class="p">[</span><span class="s2">&#34;TotalSF&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s2">&#34;TotalBsmtSF&#34;</span><span class="p">]</span> <span class="o">+</span> <span class="n">train</span><span class="p">[</span><span class="s2">&#34;GrLivArea&#34;</span><span class="p">]</span>
<span class="n">test</span><span class="p">[</span><span class="s2">&#34;TotalSF&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s2">&#34;TotalBsmtSF&#34;</span><span class="p">]</span> <span class="o">+</span> <span class="n">test</span><span class="p">[</span><span class="s2">&#34;GrLivArea&#34;</span><span class="p">]</span>
</code></pre></div><p>We will also take a look at the skewness and the kurtosis of the numerical feature to see if they are (at least somehow close to) normally distributed.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">num_cols</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">!=</span> <span class="s2">&#34;object&#34;</span>
<span class="n">skewness_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">num_cols</span><span class="p">]</span><span class="o">.</span><span class="n">skew</span><span class="p">())</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> 
<span class="n">skewness_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">num_cols</span><span class="p">]</span><span class="o">.</span><span class="n">skew</span><span class="p">())</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">skewness_train</span><span class="p">,</span> <span class="n">skewness_test</span>
</code></pre></div><p>We see that there are some variables with a very high skewness in the training data as well as in the test data. The skewness of a distribution is always compared to the skewness of the normal distribution, which is zero. To get our data to look more normal, we will log(feature + 1) transform it.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># log1p transforming the training data</span>
<span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">num_cols</span><span class="p">[</span><span class="n">num_cols</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
    <span class="n">train</span><span class="p">[</span><span class="n">var</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">var</span><span class="p">])</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">num_cols_test</span> <span class="o">=</span> <span class="n">num_cols</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="s2">&#34;SalePrice&#34;</span><span class="p">)</span>
<span class="c1"># log1p transfroming the test data</span>
<span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">num_cols_test</span><span class="p">[</span><span class="n">num_cols_test</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
    <span class="n">test</span><span class="p">[</span><span class="n">var</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">var</span><span class="p">])</span>
</code></pre></div><h2 id="2-model-selection">2. Model selection</h2>
<p>We continue by building a Machine Learning Pipeline using Scikit Learn. A pipeline object sequentially applies a list of transformers and a final estimator.</p>
<p>We will play around with the KNearestNeighbor and RandomForest algorithms, tune their hyperparameters using Cross Validation and pick the best performing one.</p>
<p>Before we can work with our data, we first need to create separate dataframes containing our feature variables and the target variable. This needs to be done only for our training data since it is our aim to predict SalePrice for the test data, which is why it is not contained in this data.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">SalePrice</span><span class="o">.</span><span class="n">values</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&#34;SalePrice&#34;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>To make predictions and to fit models, the last step that has to be done is to convert all categorical features into numeric ones. This way scikit-learn can handle them. We do this by using pandas&rsquo; get_dummies() function. To make sure we end up with the same number of columns in both the training and the test dataset we first concatenate both, then apply get_dummies() and then separate them again.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># creating a dummy to distinguish between train and test data</span>
<span class="n">X</span><span class="p">[</span><span class="s2">&#34;train&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">test</span><span class="p">[</span><span class="s2">&#34;train&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># concatenating dataframes and creating dummies from categorical features</span>
<span class="n">combined</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">test</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">combined</span><span class="p">,</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>


</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;train&#34;</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&#34;train&#34;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;train&#34;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&#34;train&#34;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Before training and tuning our models we separate the data into training and test data to evaluate them.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><h3 id="21-k-nearest-neighbors-knn">2.1 k-nearest Neighbors (KNN)</h3>
<p>We will start with a relatively simple algorithm - k-nearest Neighbor or KNN.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
</code></pre></div><p>Looking at the data, we notice that are our feature variables&rsquo; ranges vary substantially between each other. Therefore, we will add a transformer to our pipeline which standardizes the data. Standardization centers each variable around zero with unit variance. This is done by subtracting the means from each feature and dividing by its standard deviation.</p>
<p>After that we instantiate our KNN estimator, create a list containing the steps applied by the pipeline and then defining the pipeline.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># instantiate the scaling transformator </span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()</span>
<span class="c1"># instantiate the KNN estimator</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="c1"># creating a list containing the steps the pipeline is to apply</span>
<span class="n">steps_knn</span> <span class="o">=</span>  <span class="p">[(</span><span class="s2">&#34;scaler&#34;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;knn&#34;</span><span class="p">,</span> <span class="n">knn</span><span class="p">)]</span>
<span class="c1"># define the pipeline object</span>
<span class="n">pipeline_knn</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps_knn</span><span class="p">)</span>
</code></pre></div><p>The KNN algorithm has one parameter that can and should be tuned, which is the number of neighbors that should be considered. We will therefore define a dictionary containing all hyperparameters that should be tuned and define the different values that should be tested.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">neighbors</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;knn__n_neighbors&#34;</span><span class="p">:</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">21</span><span class="p">))}</span>
</code></pre></div><p>Next, we are going to set up our Cross Validation (CV) object using 5-fold CV and fit it to our data.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv_knn</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline_knn</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv_knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv_knn</span><span class="o">.</span><span class="n">best_params_</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">cv_knn</span><span class="o">.</span><span class="n">refit</span><span class="p">)</span>
</code></pre></div><p>From the above output we conclude that the default of the argument &ldquo;refit&rdquo; is &ldquo;True&rdquo;. This means that, by default, our CV pipeline automatically refits the model on the entire training set using the best parameters found by CV, which is 9 in our case. Therefore, we can now directly use the CV object to make predictions for unseen data.</p>
<p>As in the Kaggle challenge we will use the log of the root mean squared error metric to evaluate our model&rsquo;s performance on the test set. To do this we first need to predict the sales price for the unseen test data.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">cv_knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">rmse_knn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;The KNN algorithm with &#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">cv_knn</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&#34; yields a (log) RMSE of: &#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rmse_knn</span><span class="p">))</span>
</code></pre></div><h3 id="22-random-forest">2.2 Random forest</h3>
<p>After trying out the KNN algorithm, we now continue with the Random Forest Algorithm. Using similar steps as before we will build up a pipeline object applying the transformations and the estimation on our dataset sequentially automatically.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># instantiate the RandomForest Regressor</span>
<span class="n">rf_reg</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="c1"># creating a list containing the steps the pipeline should apply</span>
<span class="n">steps_rf</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&#34;scaler&#34;</span><span class="p">,</span> <span class="n">scaler</span><span class="p">),</span> <span class="p">(</span><span class="s2">&#34;rf_reg&#34;</span><span class="p">,</span> <span class="n">rf_reg</span><span class="p">)]</span>
<span class="c1"># create the pipeline object</span>
<span class="n">pipeline_rf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps_rf</span><span class="p">)</span>
</code></pre></div><p>For Random Forests there are a large number of hyperparameters that can be tuned. In this project we are going to tune the number of trees in the random forest [n_estimators], the number of features considered at every split [max_features], the maximum number of levels in a tree [max_depth], the minimum number of samples required to split a node [min_samples_split], the minimum number of observations required at each leaf node [min_samples_leaf] and if bootstrap should be used training each tree [bootstrap].</p>
<p>We first define the ranges for each hyperparameter that should be considered in our CV tuning and then create our search grid containing the before created lists.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">n_estimators</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">2001</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;auto&#34;</span><span class="p">,</span> <span class="s2">&#34;sqrt&#34;</span><span class="p">,</span> <span class="s2">&#34;log2&#34;</span><span class="p">]</span>
<span class="n">max_depth</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">max_depth</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>
<span class="n">min_samples_split</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="n">bootstrap</span> <span class="o">=</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">]</span>
<span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;rf_reg__n_estimators&#34;</span><span class="p">:</span> <span class="n">n_estimators</span><span class="p">,</span>
              <span class="s2">&#34;rf_reg__max_features&#34;</span><span class="p">:</span> <span class="n">max_features</span><span class="p">,</span>
              <span class="s2">&#34;rf_reg__max_depth&#34;</span><span class="p">:</span> <span class="n">max_depth</span><span class="p">,</span>
              <span class="s2">&#34;rf_reg__min_samples_split&#34;</span><span class="p">:</span> <span class="n">min_samples_split</span><span class="p">,</span>
              <span class="s2">&#34;rf_reg__min_samples_leaf&#34;</span><span class="p">:</span> <span class="n">min_samples_leaf</span><span class="p">,</span>
              <span class="s2">&#34;rf_reg__bootstrap&#34;</span><span class="p">:</span> <span class="n">bootstrap</span><span class="p">}</span>
</code></pre></div><p>Testing all possible combinations of hyperparameters would amount to testing 10*3*11*3*3*2 = 5,940 combinations, so instead of testing all of these we will use RandomizedSearchCV to select randomly from our defined distributions which combinations are tested.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="n">cv_random_rf</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">pipeline_rf</span><span class="p">,</span> <span class="n">param_dist</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>Just as before with the KNN algorithm we now can fit the pipeline to our data.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv_random_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv_random_rf</span><span class="o">.</span><span class="n">best_params_</span>
</code></pre></div><p>Based on the chosen parameters from RandomizedSearchCV we can now manually decrease the range of the hyperparameters to be tested and use GridSearchCV as before to find the best parameters for our model.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">n_estimators_2</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">301</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">max_depth_2</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">81</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">max_features_2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;sqrt&#34;</span><span class="p">]</span>
<span class="n">min_samples_split_2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">min_samples_leaf_2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">bootstrap_2</span> <span class="o">=</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">]</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;rf_reg__n_estimators&#34;</span><span class="p">:</span> <span class="n">n_estimators_2</span><span class="p">,</span>
              <span class="s2">&#34;rf_reg__max_features&#34;</span><span class="p">:</span> <span class="n">max_features_2</span><span class="p">,</span>
              <span class="s2">&#34;rf_reg__max_depth&#34;</span><span class="p">:</span> <span class="n">max_depth_2</span><span class="p">,</span>
              <span class="s2">&#34;rf_reg__min_samples_split&#34;</span><span class="p">:</span> <span class="n">min_samples_split_2</span><span class="p">,</span>
              <span class="s2">&#34;rf_reg__min_samples_leaf&#34;</span><span class="p">:</span> <span class="n">min_samples_leaf_2</span><span class="p">,</span>
              <span class="s2">&#34;rf_reg__bootstrap&#34;</span><span class="p">:</span> <span class="n">bootstrap_2</span><span class="p">}</span>
<span class="n">param_grid</span>
</code></pre></div><p>We can now instantiate and then fit our GridSearchCV object as before.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv_rf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline_rf</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cv_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv_rf</span><span class="o">.</span><span class="n">best_params_</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">cv_rf</span><span class="o">.</span><span class="n">refit</span><span class="p">)</span>
</code></pre></div><p>We see that the parameters chosen by RandomizedSearchCV were already pretty good and did not really need that much of a finetuning. Furthermore, the GridSearchCV function also already refit the RandomForest model on the whole training data using the parameters that are found to work best.</p>
<p>As before, we now can create predictions for our hold out test set and evaluate our model using log RMSE as before.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="n">y_pred_rf</span> <span class="o">=</span> <span class="n">cv_rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">rmse_rf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred_rf</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;The RandomForest algorithm with &#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">cv_rf</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&#34; yields a (log) RMSE of: &#34;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rmse_rf</span><span class="p">))</span>
</code></pre></div><p>Great! We see a clear improvement in our evaluation metric, i.e. the RandomForest Regressor performs much better than the KNN tried out first.</p>
<h2 id="3-making-predictions-and-creating-the-submission-file">3. Making predictions and creating the submission file</h2>
<p>Since we want to check our model&rsquo;s performance in comparison with other people&rsquo;s models, we need to make predictions for the test dataset provided by Kaggle and upload our results to Kaggle. According to the challenge&rsquo;s description, our submission should be a file containing the observation&rsquo;s ID and the predicted SalesPrice. We first create a dataframe and then save it as a .csv file.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">ident</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1461</span><span class="p">,</span> <span class="mi">2920</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ident</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test_pred</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cv_rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">ident</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_pred</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">submiss</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;Id&#34;</span><span class="p">:</span> <span class="n">ident</span><span class="p">,</span> <span class="s2">&#34;SalePrice&#34;</span><span class="p">:</span> <span class="n">test_pred</span><span class="p">}</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">submission</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">submiss</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">submission</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span>
<span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span>
<span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span>
<span class="k">print</span><span class="p">(</span><span class="n">submission</span><span class="o">.</span><span class="n">tail</span><span class="p">())</span>
</code></pre></div><p>Looking at our submission file we see that we get kind of strange predictions for the SalePrice variable. This is due to the log(x+1) transformation we did at first. We therefore have to get the data into the desired format, which can be done using the inverse of the log(x+1) transformation which is an exp(x)-1 transformation.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">submission</span><span class="o">.</span><span class="n">SalePrice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="n">submission</span><span class="o">.</span><span class="n">SalePrice</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">submission</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;submission_rf2.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">submission</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&#34;submission2020.csv&#34;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div><p>The submission files can then be uploaded to Kaggle to check how well the models perform on the unseen test dataset.</p>

            </div></div>
        
        <div>
            <hr style="height:1px;border-width:0;background-color:rgba(0,0,0,0.15)">
        </div>

        <div class="post-content-text" id="fb_comments_container">
                <h2>Comments</h2>
                <script src="https://utteranc.es/client.js" 
        repo="tlary/tlary.github.io"
        issue-term="pathname"
        theme="github-light" 
        
        crossorigin="anonymous" 
        async>
</script>
            </div>
    </div>
    


        </div>
    </div>
</div>

<script type="text/javascript"
        src="../../js/jquery.min.86b1e8f819ee2d9099a783e50b49dff24282545fc40773861f9126b921532e4c.js"
        integrity="sha256-hrHo&#43;BnuLZCZp4PlC0nf8kKCVF/EB3OGH5EmuSFTLkw="
        crossorigin="anonymous"></script>




<script type="text/javascript"
        src="../../js/bundle.min.0f9c74cb78f13d1f15f33daff4037c70354f98acfbb97a6f61708966675c3cae.js"
        integrity="sha256-D5x0y3jxPR8V8z2v9AN8cDVPmKz7uXpvYXCJZmdcPK4="
        crossorigin="anonymous"></script>

<script type="text/javascript"
        src="../../js/medium-zoom.min.92f21c856129f84aeb719459b3e6ac621a3032fd7b180a18c04e1d12083f8aba.js"
        integrity="sha256-kvIchWEp&#43;ErrcZRZs&#43;asYhowMv17GAoYwE4dEgg/iro="
        crossorigin="anonymous"></script>
<link rel="stylesheet"
              href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
              integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq"
              crossorigin="anonymous"><script defer
                src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
                integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz"
                crossorigin="anonymous"></script><script defer
                src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
                integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
                crossorigin="anonymous"
                onload="renderMathInElement(document.body);"></script></body>

</html>